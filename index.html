<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension">
  <meta name="keywords"
    content="HiREST, Information Retrieval, Video Search, Video Retrieval, Moment Retrieval, Moment Segmentation, Step Captoining">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GUIDE (IJCAI 2024)</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div> -->
  </nav>

  <!-- Abhay Zala * 1 Jaemin Cho * 1 Satwik Kottur 2 Xilun Chen2 Barlas Oguz 2Yashar Mehdad 2Mohit Bansal1 -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GUIDE: A Guideline-Guided Dataset for Instructional Video
              Comprehension (IJCAI 2024)
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Jiafeng Liang<sup>1</sup>,</span>
              <!-- <a href="https://aszala.com/">Abhay Zala</a><sup>*</sup> <sup>1</sup>,</span> -->
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Shixin Jiang<sup>1</sup>,</span>
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Zekun Wang<sup>1</sup>,</span>
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Haojie Pan<sup>3</sup>,</span>
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Zerui Chen<sup>1</sup>,</span>
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Zheng Chu<sup>1</sup>,</span>
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Ming Liu<sup>1,2*</sup>,</span>
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Ruiji Fu<sup>3</sup>,</span>
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Zhongyuan Wang<sup>3</sup>,</span>
              <span class="author-block">
                <!-- <a href="https://j-min.io/">Shixin Jiang</a><sup>*</sup> <sup>1</sup>,</span> -->
                Bing Qin<sup>1,2</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Harbin Institute of Technology, Harbin, China</span>
              <span class="author-block"><sup>2</sup>Peng Cheng Laboratory, Shenzhen, China</span>
              <span class="author-block"><sup>3</sup>Kuaishou Technology, Beijing, China</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.18227" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/jfliang3324/GUIDE"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <h4 class="subtitle has-text-centered">
    <b>If you are interested in our dataset, you can register your information through the form. We will send the
      dataset to
      your email address later.</b><a
      href="https://docs.google.com/forms/d/e/1FAIpQLScYb2Y54LegHrOdLsz3fAknvU1-u8qCKrnSmVovNXD93VMNFQ/viewform?usp=sf_link">Google
      Form</a>
  </h4>

  <br>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
        <!-- Image teaser -->
        <!-- <embed src="./static/images/main_figure.pdf" alt="Teaser" width="100%"> -->



        <!-- </h2> -->

        <img src="./static/images/overview_guide.png" alt="Teaser" width="100%">
        <!-- <h2 class="subtitle has-text-centered"> -->

        <div class="content has-text-justified">
          <p align="center"><b>Overview of the Gulde dataset.</b></p> <br>
          The GUIDE consists of 560 task queries, each containing an average of 6.2 task-related videos.
          These instructional videos are divided into specific steps with timestamps and text descriptions (yellow
          area).
          Additionally, each task contains a
          set of guideline steps representing a common pattern shared by all task-related videos (purple area).
        </div>

      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              There are substantial instructional videos on the Internet, which provide us tutorials for
              completingvarious tasks.
              Existing instructional video datasetsonly focus on specifc steps at the video level, lack.ing experiential
              guidelines at
              the task level. whichcan lead to beginners struggling to learn new tasksdue to the lack of relevant
              experience.
              Moreoverthe specific steps without guidelines are trivial andunsystematic, making it diffcult to provide a
              cleartutorial.
            </p>
            <p>
              To address these problems, we present theGUIDE (Guideline-Guided) dataset, which
              contains3.1K videos of
              600 instructional tasks in 8 domainsrelated to our daily life. Specifcally, we annotateeach instructional
              task with a
              guideline, representinga common pattern shared by all task-related videosOn this basis, we annotate
              systematic specifc
              steps.including their associated guideline steps, specifcstep descriptions and timestamps.
            </p>
            <p>
              Our proposed benchmark consists of three sub-tasks to evaluatecomprehension ability of models:(1) Step
              Captioning: models have to generate captions for specifcsteps from videos.(2)Guideline Summarization:
              models have to mine the common pattern
              in task related videos and summarize a guideline from them(3)Guideline-Guided Captioning: models have
              togenerate captions
              for specifc steps under the guideof guideline. We evaluate plenty of foundationmodels with GUIDE and
              perform in-depth
              analysis. Given the diversity and practicality of GUIDEwe believe that it can be used as a better
              benchmarkfor
              instructional video comprehension.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
      <!--/ Paper video. -->
    </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Construction Pipeline</h2>

          <h3 class="title is-4">Video Collection</h3>
          <div class="content has-text-justified">
            In this stage, we aim to collect a large number of high-qualityinstructional videos. To ensure the
            widely-used of the GUIDE.we collect videos from 560 different instructional tasks acrossthe 8 most common
            domains in our daily life. We require annotators to collect videos containing explicit instructional
            stepsand clearly defined time boundaries between these steps. Tofurther enhance the practicality of the
            dataset, we also require the collected videos that include detailed video subtitles, i.e.each step
            accompanied by corresponding voice explanations.
          </div>
          <h3 class="title is-4">Automatic Annotation</h3>
          <div class="content has-text-justified">
            The automatic annotation frame work contains two stages: Specific Steps Generation and Guideline Steps
            Generation.
          </div>
          <img src="./static/images/automatic_annnotation.png" alt="Teaser" width="100%">
          <h3 class="title is-4">Manual Annotation</h3>
          <div class="content has-text-justified">
            We employ an expert in each domain to adjust all guideline steps and require the annotators to refine the
            specific steps generated by GPT-3.5-turbo and annotate the timestamps of steps by watching videos.
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Guide Dataset</h2>


          <h3 class="title is-4">Comparison of Guide and other video datasets with annotations</h3>

          <div class="content has-text-justified">
            GUIDE covers videos of 1) various domains, 2) with many step annotations per video, and 3) high-quality
            step captions written by human annotators and gpt-3.5-turto.
          </div>


          <img src="./static/images/comparsion_dataset_table.png" alt="Teaser" width="100%">

          <!-- Add Space -->
          <br><br>

          <h3 class="title is-4">Task Definition</h3>
          <div class="content has-text-justified">
            <strong>
              Step Caption
            </strong><br>
            The step captioning task evaluates the models’capabilities tounderstand the procedural temporal knowledge of
            the instructional video. In this task, models have to generate a set ofinstructional step captions.<br>
            <strong>
              Guideline Summarization
            </strong><br>
            The guideline summarization task evaluates the models’capabilities to analyze correlations across videos. In
            this task models have to mine the common pattern in task-related videosand summarize a guideline from
            them.<br>
            <strong>
              Guideline-Guided Captioning
            </strong><br>
            To explore the impact of guidelines on step captioning, wepropose the guideline-guided captioning task. In
            this task,models have to generate specific step captions under the guideof guideline.<br>
          </div>
          <h3 class="title is-4">Video Category Distribution</h3>
          <div class="content has-text-justified">
            The videos and text queries are collected from the HowTo100M dataset.
            There are a wide variety of categories for HiREST videos.
            The most frequent categories are “Hobbies and Crafts”, “Food and Entertaining”, and “Home and Garden”.
          </div>


          <img src="./static/images/category_distribute.png" alt="Teaser" width="100%"><!--todo-->

          <br><br>

          <h3 class="title is-4">Dataset Case</h3>
          <div class="content has-text-justified">
            We evaluate three video foundation models on GUIDE:VideoChat , Video-LLaMA and mPLUG-Owl. We evaluate four
            language
            foundation models on GUIDE:GPT-3.5-turbo ,GPT-4 ,Flan-T5-XXL and Vicuna-13B.
          </div>
          <img src="./static/images/case_guide.png" alt="Teaser" width="100%">
          <!-- <h2 class="subtitle has-text-centered"> -->
          <div class="content has-text-justified">
            Comparison of foundation models and ground-truth annotation for step captioning, guideline summarization and
            guideline-guided captioning. Green, yellow, and red text denote ‘correct’, ‘partiallycorrect’,and ‘wrong’
            respectively.
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- 
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">



      <div class="column">
        <div class="content">
          <h3 class="title is-4">Step words</h3>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <img src="./static/images/step_caption_sunburst.png" alt="Teaser" width="60%">

        </div>
      </div>

      <div class="column">
        <h3 class="title is-4">Matting</h3>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <img src="./static/images/word_distribution_combined.png" alt="Teaser" width="40%">
          </div>

        </div>
      </div>
    </div>

  </div>
</section> -->




  <!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hierarchical Video Information Retrieval</h2>
        <div class="content has-text-justified">

        <p>
          
        </p>

        </div>

        <img src="./static/images/model_vs_gt_generation_example.png" alt="Teaser" width="100%">

      </div>
    </div>
  </div>
</section> -->



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      Please cite our paper if you use our dataset and/or method in your projects.
      <br><br>

      <pre><code>@misc{liang2024guideguidelineguideddatasetinstructional,
      title={GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension},
      author={Jiafeng Liang and Shixin Jiang and Zekun Wang and Haojie Pan and Zerui Chen and Zheng Chu and Ming Liu and Ruiji
      Fu and Zhongyuan Wang and Bing Qin},
      year={2024},
      eprint={2406.18227},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.18227},
      }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <!-- <div class="column is-8"> -->
        <div class="content">
          <!-- <p> -->
          The webpage was adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          <!-- </p> -->
        </div>
        <!-- </div> -->
      </div>
    </div>
  </footer>

</body>

</html>